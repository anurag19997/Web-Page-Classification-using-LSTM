{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kamal Dev Sharma\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "import nltk\n",
    "from random import shuffle\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\Users\\Kamal Dev Sharma\\Anaconda3\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import theano\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'theano'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag=sorted(train.Tag.unique())\n",
    "tagmapping=dict(zip(tag,np.arange(0,len(tag))))\n",
    "train['Tag']=train['Tag'].map(tagmapping).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(url,title):\n",
    "    \n",
    "    pattern=r'//.*'\n",
    "    urls=[]\n",
    "    for i in url:\n",
    "        b=re.findall(pattern,i)\n",
    "        c=b[0][2:]\n",
    "        d=re.split('\\.|/|-|_',c)\n",
    "        e=''\n",
    "        for i in d:\n",
    "            e+=str(i).lower()\n",
    "            e+=' '\n",
    "        e = ''.join([i for i in e if not i.isdigit()])\n",
    "        f=''\n",
    "        for i in e:\n",
    "            if i.isalpha() or i==' ':\n",
    "                f+=i\n",
    "            else:\n",
    "                f+=' '\n",
    "        urls.append(f)\n",
    "        \n",
    "    titles=[]\n",
    "    for e in title:    \n",
    "        e = ''.join([i for i in e if not i.isdigit()])\n",
    "        f=''\n",
    "        for i in e:\n",
    "            if i.isalpha() or i==' ':\n",
    "                #print(i)\n",
    "                f+=i\n",
    "            else:\n",
    "                f+=' '\n",
    "        f = re.sub(' +',' ',f) # replace series of spaces with single space\n",
    "        titles.append(f)\n",
    "    \n",
    "    data=[]\n",
    "    for i in range(len(title)):\n",
    "        s=titles[i]+' '+urls[i]\n",
    "        data.append(str(s))\n",
    "                    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_title():\n",
    "    pattern=r'<title>.*</title>'\n",
    "\n",
    "    zf = zipfile.ZipFile('train.zip') \n",
    "    df = pd.read_csv(zf.open('html_data.csv'),chunksize=1)\n",
    "\n",
    "    title=[]\n",
    "    chunksize = 1\n",
    "    for chunk in pd.read_csv(zf.open('html_data.csv'), chunksize=chunksize):\n",
    "        a=chunk.Html\n",
    "        idd=chunk.Webpage_id\n",
    "        c=re.findall(pattern,a.iloc[0])\n",
    "        if len(c)!=0:\n",
    "            title.append([idd.iloc[0],c[0][7:-8]])\n",
    "        else:\n",
    "            title.append([idd.iloc[0],' '])\n",
    "    return(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#title=preprocess_title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_data=pd.DataFrame(title)\n",
    "# title_data.columns=['Webpage_id','Html']\n",
    "\n",
    "# new_data=pd.merge(train,title_data,on=['Webpage_id'],how='left')\n",
    "# new_data.to_csv('processed_train_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('processed_train_data.csv',lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag=sorted(data.Tag.unique())\n",
    "tagmapping=dict(zip(tag,np.arange(0,len(tag))))\n",
    "data['Tag']=data['Tag'].map(tagmapping).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Webpage_id</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Url</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Html</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>www.fiercepharma.com</td>\n",
       "      <td>http://www.fiercepharma.com/marketing/tecfidera-gilenya-and-aubagio-s-3-way-battle-for-ms-share-about-to-get-more-interesting</td>\n",
       "      <td>4</td>\n",
       "      <td>Tecfidera, Gilenya and Aubagio&amp;#039;s 3-way battle for MS share is about to heat up | FiercePharma\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>www.fiercepharma.com</td>\n",
       "      <td>http://www.fiercepharma.com/pharma/novo-equipped-to-weather-storm-u-s-diabetes-market-ceo-says</td>\n",
       "      <td>4</td>\n",
       "      <td>Novo equipped to weather the storm in the U.S. diabetes market, CEO says | FiercePharma\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>www.fiercepharma.com</td>\n",
       "      <td>http://www.fiercepharma.com/pharma/another-exec-departs-troubled-endo-and-time-it-s-for-another-drugmaker</td>\n",
       "      <td>4</td>\n",
       "      <td>Another exec departs troubled Endo--and this time, it&amp;#039;s for another drugmaker | FiercePharma\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>www.fiercepharma.com</td>\n",
       "      <td>http://www.fiercepharma.com/pharma/teva-buy-biosim-specialist-celltrion-it-wouldn-t-say-no</td>\n",
       "      <td>4</td>\n",
       "      <td>Would Teva buy Korea&amp;#039;s Celltrion to beef up in biosimilars? It wouldn&amp;#039;t say no | FiercePharma\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>www.fiercepharma.com</td>\n",
       "      <td>http://www.fiercepharma.com/marketing/actress-marissa-tomei-partners-allergan-restasis-to-drive-dry-eye-awareness</td>\n",
       "      <td>4</td>\n",
       "      <td>Restasis-maker Allergan recruits actress Marisa Tomei to drive dry eye awareness | FiercePharma\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Webpage_id                Domain  \\\n",
       "0  1           www.fiercepharma.com   \n",
       "1  2           www.fiercepharma.com   \n",
       "2  3           www.fiercepharma.com   \n",
       "3  4           www.fiercepharma.com   \n",
       "4  5           www.fiercepharma.com   \n",
       "\n",
       "                                                                                                                             Url  \\\n",
       "0  http://www.fiercepharma.com/marketing/tecfidera-gilenya-and-aubagio-s-3-way-battle-for-ms-share-about-to-get-more-interesting   \n",
       "1  http://www.fiercepharma.com/pharma/novo-equipped-to-weather-storm-u-s-diabetes-market-ceo-says                                  \n",
       "2  http://www.fiercepharma.com/pharma/another-exec-departs-troubled-endo-and-time-it-s-for-another-drugmaker                       \n",
       "3  http://www.fiercepharma.com/pharma/teva-buy-biosim-specialist-celltrion-it-wouldn-t-say-no                                      \n",
       "4  http://www.fiercepharma.com/marketing/actress-marissa-tomei-partners-allergan-restasis-to-drive-dry-eye-awareness               \n",
       "\n",
       "   Tag  \\\n",
       "0  4     \n",
       "1  4     \n",
       "2  4     \n",
       "3  4     \n",
       "4  4     \n",
       "\n",
       "                                                                                                      Html\\r  \n",
       "0  Tecfidera, Gilenya and Aubagio&#039;s 3-way battle for MS share is about to heat up | FiercePharma\\r       \n",
       "1  Novo equipped to weather the storm in the U.S. diabetes market, CEO says | FiercePharma\\r                  \n",
       "2  Another exec departs troubled Endo--and this time, it&#039;s for another drugmaker | FiercePharma\\r        \n",
       "3  Would Teva buy Korea&#039;s Celltrion to beef up in biosimilars? It wouldn&#039;t say no | FiercePharma\\r  \n",
       "4  Restasis-maker Allergan recruits actress Marisa Tomei to drive dry eye awareness | FiercePharma\\r          "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(train):\n",
    "    grp=train.groupby('Tag')\n",
    "    \n",
    "    test_set=[]\n",
    "    train_set=[]\n",
    "    test_tag_set=[]\n",
    "    train_tag_set=[]\n",
    "    count=0\n",
    "    for unique_tags in train.Tag.unique():\n",
    "        tag=grp.get_group(unique_tags)\n",
    "        s=set(tag.Domain)\n",
    "        count+=len(s)\n",
    "        a=list(s)\n",
    "        #shuffle(a)\n",
    "        for i in range(len(a)):\n",
    "            if (i+1)%3==0:\n",
    "                test_set.append(a[i])\n",
    "                test_tag_set.append(unique_tags)\n",
    "            else:\n",
    "                train_set.append(a[i])\n",
    "                train_tag_set.append(unique_tags)\n",
    "    \n",
    "    \n",
    "    train_domain=pd.DataFrame(train_set)\n",
    "    train_domain.columns=['Domain']\n",
    "    train_domain['Tag']=train_tag_set\n",
    "    train_data=pd.merge(train_domain,train,on=['Domain','Tag'],how='left')\n",
    "    \n",
    "    val_domain=pd.DataFrame(test_set)\n",
    "    val_domain.columns=['Domain']\n",
    "    val_domain['Tag']=test_tag_set\n",
    "    val_data=pd.merge(val_domain,train,on=['Domain','Tag'],how='left')\n",
    "    \n",
    "    return(train_data,val_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,val_data = train_val_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_urls=preprocess(train_data.Url,train_data['Html\\r'])\n",
    "train_target=train_data.Tag.values\n",
    "\n",
    "val_urls = preprocess(val_data.Url,val_data['Html\\r'])\n",
    "val_target = val_data.Tag.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def more_prep(urls):\n",
    "    url=[]\n",
    "    stop_words = set(['','i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]) \n",
    "\n",
    "    for i in urls:\n",
    "        c = re.sub(r\"[^a-z A-Z]+\", \"\", i)\n",
    "        words = c.split(' ')\n",
    "        filtered_sentence = [w for w in words if not w in stop_words]   \n",
    "        url.append(filtered_sentence)\n",
    "        \n",
    "    return(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url=more_prep(train_urls)\n",
    "val_url=more_prep(train_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe(target):\n",
    "    n=np.arange(0,9).reshape((-1,1))\n",
    "    #print(n)\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc.fit(n)\n",
    "    y_enc=enc.transform(np.array(target).reshape((target.shape[0],1))).toarray()\n",
    "    return(y_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34476,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ohe=to_categorical(train_target, num_classes=9)\n",
    "y_val_ohe=to_categorical(val_target, num_classes=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53447"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('urls.pkl', 'rb') as f:\n",
    "    urls = pickle.load(f)\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=more_prep(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = Word2Vec(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14068"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mod.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(text , mod, max_len):\n",
    "    m = len(text)                                 \n",
    "    text_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):                      \n",
    "        j = 0\n",
    "        for w in text[i]:\n",
    "            if j==max_len:\n",
    "                break\n",
    "            if w not in mod.wv.vocab:\n",
    "                continue\n",
    "            text_indices[i, j] = mod.wv.vocab[w].index  # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            j = j + 1\n",
    "            \n",
    "    return text_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(mod):\n",
    "    vocab_len = len(mod.wv.vocab) + 1                 \n",
    "    \n",
    "    emb_dim = mod[\"hi\"].shape[0]\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    index=0\n",
    "    for word in mod.wv.vocab:\n",
    "        emb_matrix[index, :] = mod[word]\n",
    "        index+=1\n",
    "        \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def webpage_model(input_shape,mod):\n",
    "    sentence_indices = Input(shape=input_shape)\n",
    "    embedding_layer =  pretrained_embedding_layer(mod)\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    X = Dropout(0.5)(X)\n",
    "    #X = Flatten()(X)\n",
    "    X = LSTM(128)(X)\n",
    "    X = Dropout(0.5)(X)  # Add dropout with a probability of 0.5\n",
    "    X = Dense(9, activation='softmax')(X)\n",
    "    X =  Activation('softmax')(X)\n",
    "    model = Model(sentence_indices, X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kamal Dev Sharma\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Kamal Dev Sharma\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = webpage_model((max_len,),mod)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from keras import optimizers\n",
    "#Keras default : lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0.\n",
    "#adm = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam' , metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_indices = sentences_to_indices(train_url, mod,max_len)\n",
    "x_val_indices = sentences_to_indices(val_url, mod,max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(x_train_indices, y_train_ohe, epochs = 5, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_enc=[]\n",
    "for i in pred:\n",
    "    pred_enc.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
